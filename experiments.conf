# Word embeddings.
word2vec_300d {
  path = word2vec.txt
  size = 300
}
word2vec_300d_filtered {
  path = korean8_pd_NER/word2vec.korean8_pd_NER.txt.filtered
  size = 300
}
word2vec_300d_2w {
  path = word2vec.txt
  size = 300
}
# YES NER
# Main configuration.
best {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 0
  char_vocab_path = "korean8_pd_NER/char_vocab.korean8_pd_NER.txt"
  context_embeddings = ${word2vec_300d_filtered}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 100 #200
  contextualization_layers = 3
  ffnn_size = 100 #150
  ffnn_depth = 2
  feature_size = 10 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = korean8_pd_NER/train.korean8_pd_NER.jsonlines
  eval_path = korean8_pd_NER/dev.korean8_pd_NER.jsonlines
  conll_eval_path = korean8_pd_NER/dev.korean8_pd_NER.v4_gold_conll
  lm_path = korean8_pd_NER/elmo_cache.korean8_pd_NER.hdf5
  #train_path = data/train.pilot2.jsonlines
  #eval_path = data/dev.pilot2.jsonlines
  #conll_eval_path = data/dev.pilot2.v4_gold_conll
  #lm_path = elmo_cache.pilot2.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

# For evaluation. Do not use for training (i.e. only for predict.py, evaluate.py, and demo.py). Rename `best` directoryream_mdinal`.
model_final = ${best} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
#  eval_path = korean8_pd_NER/test.korean8_pd_NER.jsonlines
#  conll_eval_path = korean8_pd_NER/test.korean8_pd_NER.v4_gold_conll
  eval_path = MTA02/detected-mention/test.NER5.jsonlines
  conll_eval_path = MTA02/detected-mention/test.NER5.v4_gold_conll
#  eval_path = data/test.korean_pilot1_final_pd_NER.jsonlines
#  conll_eval_path = data/test.korean_pilot1_final_pd_NER.v4_gold_conll
}


word2vec_300d_filtered0 {
  path = ./MTA02/input/word2vec.NER5.txt.filtered
  size = 300
}

MTA02_nonJosa {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = MTA02/input/nonJosa_train1345.NER5.jsonlines
  eval_path = MTA02/input/nonJosa_dev.NER5.jsonlines
  conll_eval_path = MTA02/input/nonJosa_dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_nonJosa-test = ${MTA02_nonJosa} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
#  eval_path = GOLD/GOLD.jsonlines
#  conll_eval_path = GOLD/GOLD.v4_gold_conll
  eval_path = korean8_pd_NER/whole.korean8_pd_NER.jsonlines
  conll_eval_path = korean8_pd_NER/whole.korean8_pd_NER.v4_gold_conll
}


MTA02_64 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/40/shuffled15.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_64-test = ${MTA02_64} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}



MTA02_63 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/40/shuffled14.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_63-test = ${MTA02_63} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}




MTA02_62 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/40/shuffled13.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_62-test = ${MTA02_62} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}





MTA02_61 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/40/shuffled12.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_61-test = ${MTA02_61} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}




MTA02_60 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/20/shuffled5.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_60-test = ${MTA02_60} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}






MTA02_59 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/20/shuffled4.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_59-test = ${MTA02_59} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}





MTA02_58 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/20/shuffled3.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_58-test = ${MTA02_58} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}






MTA02_57 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/20/shuffled2.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_57-test = ${MTA02_57} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}





MTA02_56 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = cheolhun/shuffle/data/20/shuffled1.train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_56-test = ${MTA02_56} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
}





MTA02_100 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = MTA02/input/train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_100-test = ${MTA02_100} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = korean8_pd_NER/whole.korean8_pd_NER.jsonlines
  conll_eval_path = korean8_pd_NER/whole.korean8_pd_NER.v4_gold_conll
}

MTA02_best {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 24
  char_vocab_path = "./MTA02/input/char_vocab.NER5.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 250 #200
  contextualization_layers = 3
  ffnn_size = 250 #150
  ffnn_depth = 2
  feature_size = 40 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = MTA02/input/train1345.NER5.jsonlines
  eval_path = MTA02/input/dev.NER5.jsonlines
  conll_eval_path = MTA02/input/dev.NER5.v4_gold_conll
  lm_path = MTA02/input/elmo_cache.NER5.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

MTA02_best-test = ${MTA02_best} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = GOLD/GOLD.jsonlines
  conll_eval_path = GOLD/GOLD.v4_gold_conll
#  eval_path = korean8_pd_NER/whole.korean8_pd_NER.jsonlines
#  conll_eval_path = korean8_pd_NER/whole.korean8_pd_NER.v4_gold_conll
}


word2vec_300d_filtered1 {
  path = ./korean8_shuffled/word2vec.korean8_pd_NER.txt.filtered
  size = 300
}

korean8_4 {
  # Computation limits.
  max_top_antecedents = 50 #50
  max_training_sentences = 50
  top_span_ratio = 0.4 #0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 0
  char_vocab_path = "./korean8_shuffled/char_vocab.korean8_pd_NER.txt"
  context_embeddings = ${word2vec_300d_filtered0}
  head_embeddings = ${word2vec_300d_2w}
  contextualization_size = 100 #200
  contextualization_layers = 3
  ffnn_size = 100 #150
  ffnn_depth = 2
  feature_size = 10 #20
  max_span_width = 30
  use_metadata = false
  use_features = true
  use_NER = true
  use_mention = false
  mention_boundary = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  mention_const = 0.5

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = korean8_shuffled/train.korean8_pd_NER.jsonlines
  eval_path = korean8_shuffled/dev.korean8_pd_NER.jsonlines
  conll_eval_path = korean8_shuffled/dev.korean8_pd_NER.v4_gold_conll
  lm_path = korean8_shuffled/elmo_cache.korean8_pd_NER.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
}

korean8_4-test = ${korean8_4} {
  context_embeddings = ${word2vec_300d}
  head_embeddings = ${word2vec_300d_2w}
  lm_path = ""
# Requires test file's path. {}.jsonlines and {}.v4_gold_conll
  eval_path = korean8_shuffled/test.korean8_pd_NER.jsonlines
  conll_eval_path = korean8_shuffled/test.korean8_pd_NER.v4_gold_conll
#  eval_path = ./MTA02/detected-mention/test.NER5.jsonlines
#  conll_eval_path = ./MTA02/detected-mention/test.NER5.v4_gold_conll
}

# Baselines.
c2f_100_ant = ${best} {
  max_top_antecedents = 100
}
c2f_250_ant = ${best} {
  max_top_antecedents = 250
}
c2f_1_layer = ${best} {
  coref_depth = 2
}
c2f_3_layer = ${best} {
  coref_depth = 3
}
distance_50_ant = ${best} {
  max_top_antecedents = 50
  coarse_to_fine = false
  coref_depth = 1
}
distance_100_ant = ${distance_50_ant} {
  max_top_antecedents = 100
}
distance_250_ant = ${distance_50_ant} {
  max_top_antecedents = 250
}
